{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 Sample Code\n",
    "\n",
    "This sample code is meant as a guide on how to use PyTorch and how to use the relevant model layers. This not a guide on how to design a network and the network in this example is intentionally designed to have poor performace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MNIST\n",
    "The `torchvision` module contains links to many standard datasets. We can load the MNIST dataset into a `Dataset` object as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241m.\u001b[39mMNIST(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Downloads into a directory ../data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m                                transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor())\n\u001b[1;32m      3\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mMNIST(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# No need to download again\u001b[39;00m\n\u001b[1;32m      4\u001b[0m                               transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.MNIST('./data', train=True, download=True,  # Downloads into a directory ../data\n",
    "                               transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=False,  # No need to download again\n",
    "                              transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dataset` object is an iterable where each element is a tuple of (input `Tensor`, target):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 <class 'torch.Tensor'> <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset), type(train_dataset[0][0]), type(train_dataset[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert images to numpy arrays and plot them with matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(train_dataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy(), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.imshow(train_dataset[0][0][0].numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Definition\n",
    "Let's instantiate a model and take a look at the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# In problem 2, we don't use the 2D structure of an image at all. Our network\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# takes in a flat vector of the pixel values as input.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     nn\u001b[38;5;241m.\u001b[39mFlatten(),  \n\u001b[1;32m      5\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m784\u001b[39m, \u001b[38;5;241m20\u001b[39m),\n\u001b[1;32m      6\u001b[0m     nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[1;32m      7\u001b[0m     nn\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.5\u001b[39m),\n\u001b[1;32m      8\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    # In problem 2, we don't use the 2D structure of an image at all. Our network\n",
    "    # takes in a flat vector of the pixel values as input.\n",
    "    nn.Flatten(),  \n",
    "    nn.Linear(784, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We also choose an optimizer and a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m      2\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could write our training procedure manually and directly index the `Dataset` objects, but the `DataLoader` object conveniently creates an iterable for automatically creating random minibatches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write our backpropagation loop, training for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1  Loss: 0.7900\n",
      "Train Epoch: 2  Loss: 0.7049\n",
      "Train Epoch: 3  Loss: 0.6281\n",
      "Train Epoch: 4  Loss: 0.6197\n",
      "Train Epoch: 5  Loss: 1.1989\n",
      "Train Epoch: 6  Loss: 0.8405\n",
      "Train Epoch: 7  Loss: 0.6611\n",
      "Train Epoch: 8  Loss: 0.6278\n",
      "Train Epoch: 9  Loss: 0.6922\n",
      "Train Epoch: 10  Loss: 0.8229\n"
     ]
    }
   ],
   "source": [
    "# Some layers, such as Dropout, behave differently during training\n",
    "model.train()\n",
    "\n",
    "for epoch in range(10):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Erase accumulated gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Weight update\n",
    "        optimizer.step()\n",
    "\n",
    "    # Track loss each epoch\n",
    "    print('Train Epoch: %d  Loss: %.4f' % (epoch + 1,  loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "We can perform forward passes through the network without saving gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0086, Accuracy: 9247/10000 (92.4700)\n"
     ]
    }
   ],
   "source": [
    "# Putting layers like Dropout into evaluation mode\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "# Turning off automatic differentiation\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        test_loss += loss_fn(output, target).item()  # Sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max class score\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('Test set: Average loss: %.4f, Accuracy: %d/%d (%.4f)' %\n",
    "      (test_loss, correct, len(test_loader.dataset),\n",
    "       100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2 Problem B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241m.\u001b[39mMNIST(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Downloads into a directory ../data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m                                transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor())\n\u001b[1;32m      3\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mMNIST(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# No need to download again\u001b[39;00m\n\u001b[1;32m      4\u001b[0m                               transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.MNIST('./data', train=True, download=True,  # Downloads into a directory ../data\n",
    "                               transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=False,  # No need to download again\n",
    "                              transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "N_epochs = 10\n",
    "\n",
    "model = nn.Sequential(\n",
    "    # In problem 2, we don't use the 2D structure of an image at all. Our network\n",
    "    # takes in a flat vector of the pixel values as input.\n",
    "    nn.Flatten(),  \n",
    "    nn.Linear(784, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Softmax(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True) \n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(N_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Erase accumulated gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Weight update\n",
    "        optimizer.step()\n",
    "\n",
    "    # Track loss each epoch\n",
    "    print('Train Epoch: %d  Loss: %.4f' % (epoch + 1,  loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
